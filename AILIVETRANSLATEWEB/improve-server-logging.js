// Script to improve server logging and add diagnostic information
const fs = require('fs');
const path = require('path');

// Read the current server file
const serverPath = path.join(__dirname, '..', 'azure-server.js');
let serverContent = fs.readFileSync(serverPath, 'utf8');

// Add enhanced logging to WebSocket message handling
const enhancedLogging = `
    ws.on('message', (data) => {
      try {
        console.log('[WebSocket] üì• Received message from client');
        console.log('[WebSocket] üìä Message type:', typeof data);
        console.log('[WebSocket] üìè Message size:', data.length || data.byteLength || 'unknown');
        
        // Check if it's a JSON message (init, language_update, etc.)
        if (typeof data === 'string') {
          const message = JSON.parse(data);
          console.log('[WebSocket] üìã Parsed JSON message:', message.type);
          console.log('[WebSocket] üîç Message details:', JSON.stringify(message, null, 2));
          
          if (message.type === 'init') {
            console.log('[WebSocket] üîß Processing init message...');
            console.log('[WebSocket] üåê Source language:', message.language);
            console.log('[WebSocket] üéØ Target language:', message.targetLanguage);
            console.log('[WebSocket] üîÑ Auto detection:', message.autoDetection);
            
            // ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ¨ŸÑÿ≥ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ŸÇÿ®ŸÑ ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ŸáŸäÿ¶ÿ©
            try {
              recognizer.stopContinuousRecognitionAsync();
              recognizer.close();
              pushStream.close();
              console.log('[Azure] üßπ Cleaned up previous session before reinitialization');
            } catch (error) {
              console.log('[Azure] ‚ö†Ô∏è No previous session to cleanup:', error.message);
            }
            
            // Handle auto detection or specific language
            const rawLanguage = message.language;
            const autoDetection = message.autoDetection || false;
            
            let language;
            if (autoDetection || !rawLanguage) {
              // Use auto detection - Azure will automatically detect from 10+ languages
              language = null;
              console.log('[Azure] üîß Initializing with AUTO DETECTION (no specific language)');
            } else {
              // Use specific language
              language = validateAzureLanguage(rawLanguage);
              console.log('[Azure] üîß Initializing with specific language:', rawLanguage, '‚Üí', language);
            }
            
            // ÿ•ŸÜÿ¥ÿßÿ° ÿ¨ŸÑÿ≥ÿ© ÿ¨ÿØŸäÿØÿ© ŸÖÿπ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©
            // ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ 16kHz ÿ®ÿØŸÑÿßŸã ŸÖŸÜ 48kHz ŸÑÿ™ÿ¨ŸÜÿ® ŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ
            pushStream = speechsdk.AudioInputStream.createPushStream(speechsdk.AudioStreamFormat.getWaveFormatPCM(16000, 16, 1));
            audioConfig = speechsdk.AudioConfig.fromStreamInput(pushStream);
            speechConfig = speechsdk.SpeechConfig.fromSubscription(AZURE_SPEECH_KEY, AZURE_SPEECH_REGION);
            
            // Set language only if not using auto detection
            if (language) {
              speechConfig.speechRecognitionLanguage = language;
            } else {
              // For auto detection, don't set a specific language
              // Azure will automatically detect from supported languages
              console.log('[Azure] üéØ Auto detection enabled - Azure will detect language automatically');
            }
            
            // ÿ™ÿ≠ÿ≥ŸäŸÜÿßÿ™ Azure ŸÑÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ chunks ŸÉÿ®Ÿäÿ±ÿ©
            speechConfig.setProperty(speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "15000");
            speechConfig.setProperty(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "10000");
            speechConfig.setProperty(speechsdk.PropertyId.SpeechServiceResponse_RequestDetailedResultTrueFalse, "true");
            
            recognizer = new speechsdk.SpeechRecognizer(speechConfig, audioConfig);
            
            // ÿ•ÿπÿØÿßÿØ ÿßŸÑŸÖÿ≥ÿ™ŸÖÿπŸäŸÜ ÿßŸÑÿ¨ÿØÿØ
            recognizer.recognizing = (s, e) => {
              console.log(\`[Azure Speech] üîÑ Partial result: "\${e.result.text}"\`);
              if (e.result.text && e.result.text.trim()) {
                console.log(\`[Azure Speech] üì§ Sending partial transcription: "\${e.result.text}"\`);
                ws.send(JSON.stringify({ type: 'transcription', text: e.result.text }));
              }
            };
            
            recognizer.recognized = (s, e) => {
              if (e.result.reason === speechsdk.ResultReason.RecognizedSpeech) {
                console.log(\`[Azure Speech] ‚úÖ Final result: "\${e.result.text}"\`);
                if (e.result.text && e.result.text.trim()) {
                  console.log(\`[Azure Speech] üì§ Sending final transcription: "\${e.result.text}"\`);
                  ws.send(JSON.stringify({ type: 'final', text: e.result.text }));
                } else {
                  console.log('[Azure Speech] Empty final result, not sending');
                }
              }
            };
            
            recognizer.canceled = (s, e) => {
              console.log('[Azure Speech] ‚ùå Recognition canceled:', e.errorDetails);
              ws.send(JSON.stringify({ type: 'error', error: e.errorDetails }));
            };
            
            recognizer.sessionStopped = (s, e) => {
              console.log('[Azure Speech] üõë Session stopped');
              ws.send(JSON.stringify({ type: 'done' }));
            };
            
            // ÿ®ÿØÿ° ÿßŸÑÿ™ÿπÿ±ŸÅ ÿßŸÑÿ¨ÿØŸäÿØ
            recognizer.startContinuousRecognitionAsync(
              () => {
                if (language) {
                  console.log('[Azure Speech] ‚úÖ Recognition started successfully with language:', language);
                  ws.send(JSON.stringify({ type: 'status', message: 'Initialized with language: ' + language }));
                } else {
                  console.log('[Azure Speech] ‚úÖ Recognition started successfully with AUTO DETECTION');
                  ws.send(JSON.stringify({ type: 'status', message: 'Initialized with AUTO DETECTION - Azure will detect language automatically' }));
                }
              },
              (error) => {
                console.log('[Azure Speech] ‚ùå Failed to start recognition:', error);
                ws.send(JSON.stringify({ type: 'error', error: 'Failed to start recognition: ' + error }));
              }
            );
          } else if (message.type === 'language_update') {
            console.log('[WebSocket] üîÑ Processing language update...');
            console.log('[WebSocket] üåê New source language:', message.sourceLanguage);
            console.log('[WebSocket] üéØ New target language:', message.targetLanguage);
            
            // ÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ¨ŸÑÿ≥ÿ© ÿßŸÑÿ≠ÿßŸÑŸäÿ© ŸÇÿ®ŸÑ ÿ™ÿ≠ÿØŸäÿ´ ÿßŸÑŸÑÿ∫ÿ©
            try {
              recognizer.stopContinuousRecognitionAsync();
              recognizer.close();
              pushStream.close();
              console.log('[Azure] üîÑ Cleaned up session before language update');
            } catch (error) {
              console.log('[Azure] ‚ö†Ô∏è No session to cleanup for language update:', error.message);
            }
            
            // Handle auto detection or specific language update
            const rawLanguage = message.sourceLanguage;
            const autoDetection = message.autoDetection || false;
            
            let language;
            if (autoDetection || !rawLanguage) {
              // Use auto detection
              language = null;
              console.log('[Azure] üîß Updating to AUTO DETECTION (no specific language)');
            } else {
              // Use specific language
              language = validateAzureLanguage(rawLanguage);
              console.log('[Azure] üîß Updating language from:', rawLanguage, '‚Üí', language);
            }
            
            // ÿ•ŸÜÿ¥ÿßÿ° ÿ¨ŸÑÿ≥ÿ© ÿ¨ÿØŸäÿØÿ© ŸÖÿπ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸÖÿ≠ÿØÿ´ÿ©
            // ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ 16kHz ÿ®ÿØŸÑÿßŸã ŸÖŸÜ 48kHz ŸÑÿ™ÿ¨ŸÜÿ® ŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ
            pushStream = speechsdk.AudioInputStream.createPushStream(speechsdk.AudioStreamFormat.getWaveFormatPCM(16000, 16, 1));
            audioConfig = speechsdk.AudioConfig.fromStreamInput(pushStream);
            speechConfig = speechsdk.SpeechConfig.fromSubscription(AZURE_SPEECH_KEY, AZURE_SPEECH_REGION);
            
            // Set language only if not using auto detection
            if (language) {
              speechConfig.speechRecognitionLanguage = language;
            } else {
              // For auto detection, don't set a specific language
              console.log('[Azure] üéØ Auto detection enabled for language update - Azure will detect language automatically');
            }
            
            // ÿ™ÿ≠ÿ≥ŸäŸÜÿßÿ™ Azure ŸÑÿßÿ≥ÿ™ŸÇÿ®ÿßŸÑ chunks ŸÉÿ®Ÿäÿ±ÿ©
            speechConfig.setProperty(speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs, "15000");
            speechConfig.setProperty(speechsdk.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs, "10000");
            speechConfig.setProperty(speechsdk.PropertyId.SpeechServiceResponse_RequestDetailedResultTrueFalse, "true");
            
            recognizer = new speechsdk.SpeechRecognizer(speechConfig, audioConfig);
            
            // ÿ•ÿπÿØÿßÿØ ÿßŸÑŸÖÿ≥ÿ™ŸÖÿπŸäŸÜ ÿßŸÑÿ¨ÿØÿØ
            recognizer.recognizing = (s, e) => {
              console.log(\`[Azure Speech] üîÑ Partial result: "\${e.result.text}"\`);
              if (e.result.text && e.result.text.trim()) {
                console.log(\`[Azure Speech] üì§ Sending partial transcription: "\${e.result.text}"\`);
                ws.send(JSON.stringify({ type: 'transcription', text: e.result.text }));
              }
            };
            
            recognizer.recognized = (s, e) => {
              if (e.result.reason === speechsdk.ResultReason.RecognizedSpeech) {
                console.log(\`[Azure Speech] ‚úÖ Final result: "\${e.result.text}"\`);
                if (e.result.text && e.result.text.trim()) {
                  console.log(\`[Azure Speech] üì§ Sending final transcription: "\${e.result.text}"\`);
                  ws.send(JSON.stringify({ type: 'final', text: e.result.text }));
                } else {
                  console.log('[Azure Speech] Empty final result, not sending');
                }
              }
            };
            
            recognizer.canceled = (s, e) => {
              console.log('[Azure Speech] ‚ùå Recognition canceled:', e.errorDetails);
              ws.send(JSON.stringify({ type: 'error', error: e.errorDetails }));
            };
            
            recognizer.sessionStopped = (s, e) => {
              console.log('[Azure Speech] üõë Session stopped');
              ws.send(JSON.stringify({ type: 'done' }));
            };
            
            // ÿ®ÿØÿ° ÿßŸÑÿ™ÿπÿ±ŸÅ ÿßŸÑÿ¨ÿØŸäÿØ
            recognizer.startContinuousRecognitionAsync(
              () => {
                console.log('[Azure Speech] ‚úÖ Language updated successfully to:', language);
                ws.send(JSON.stringify({ type: 'status', message: 'Language updated to: ' + language }));
              },
              (error) => {
                console.log('[Azure Speech] ‚ùå Failed to update language:', error);
                ws.send(JSON.stringify({ type: 'error', error: 'Failed to update language: ' + error }));
              }
            );
          } else if (message.type === 'audio') {
            // Handle audio data
            console.log('[WebSocket] üéµ Processing audio message...');
            console.log('[WebSocket] üìä Audio format:', message.format);
            console.log('[WebSocket] üìè Audio data length:', message.data ? message.data.length : 'undefined');
            
            const audioData = message.data;
            if (audioData) {
              const audioBuffer = Buffer.from(audioData, 'base64');
              console.log('[WebSocket] ‚úÖ Audio data decoded successfully');
              console.log('[WebSocket] üìè Decoded audio buffer size:', audioBuffer.length, 'bytes');
              console.log('[WebSocket] üéµ Audio format from client:', message.format);
              console.log('[WebSocket] üåê Source language:', message.sourceLanguage || 'not specified');
              console.log('[WebSocket] üéØ Target language:', message.targetLanguage || 'not specified');
              
              // ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸäÿ≥ÿ™ ŸÅÿßÿ±ÿ∫ÿ© ÿ£Ÿà ŸÇÿØŸäŸÖÿ© ÿ¨ÿØÿßŸã
              if (audioBuffer.length > 0) {
                console.log('[WebSocket] üì§ Writing audio buffer to Azure push stream...');
                pushStream.write(audioBuffer);
                console.log('[WebSocket] ‚úÖ Audio buffer written to Azure stream');
              } else {
                console.log('[WebSocket] ‚ö†Ô∏è Skipping empty audio buffer');
              }
            } else {
              console.log('[WebSocket] ‚ùå No audio data in message');
            }
          } else {
            console.log('[WebSocket] ‚ùì Unknown message type:', message.type);
          }
        } else {
          // Handle raw audio data (fallback)
          console.log('[WebSocket] üì• Received raw audio data (fallback)');
          console.log('[WebSocket] üìä Data type:', typeof data);
          console.log('[WebSocket] üìè Data size:', data.length || data.byteLength || 'unknown');
          
          // ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸÑŸäÿ≥ÿ™ ŸÅÿßÿ±ÿ∫ÿ©
          if (data && (data.length > 0 || data.byteLength > 0)) {
            console.log('[WebSocket] üì§ Writing raw audio data to Azure push stream...');
            if (data instanceof Buffer) {
              pushStream.write(data);
            } else if (data instanceof ArrayBuffer) {
              pushStream.write(Buffer.from(data));
            } else {
              pushStream.write(Buffer.from(data));
            }
            console.log('[WebSocket] ‚úÖ Raw audio data written to Azure stream');
          } else {
            console.log('[WebSocket] ‚ö†Ô∏è Skipping empty raw audio data');
          }
        }
      } catch (error) {
        console.error('[WebSocket] ‚ùå Error processing message:', error);
        console.error('[WebSocket] ‚ùå Error details:', error.message);
        console.error('[WebSocket] ‚ùå Error stack:', error.stack);
      }
    });
`;

// Replace the existing message handler with enhanced logging
const messageHandlerPattern = /ws\.on\('message', \(data\) => \{[\s\S]*?\}\);[\s\S]*?\} else \{[\s\S]*?\}[\s\S]*?\} catch \(error\) \{[\s\S]*?\}/;
const replacement = enhancedLogging;

if (messageHandlerPattern.test(serverContent)) {
  serverContent = serverContent.replace(messageHandlerPattern, replacement);
  console.log('‚úÖ Enhanced logging added to server');
} else {
  console.log('‚ùå Could not find message handler pattern in server file');
}

// Add connection logging
const connectionLogging = `
    wsServer.on('connection', (ws) => {
      console.log('[WebSocket] üîó New client connected');
      console.log('[WebSocket] üìä Client IP:', ws._socket.remoteAddress);
      console.log('[WebSocket] üìä Client port:', ws._socket.remotePort);
      console.log('[WebSocket] üìä User agent:', ws._socket.remoteAddress ? 'Unknown' : 'Direct connection');
      
      try {
        const AZURE_SPEECH_KEY = process.env.AZURE_SPEECH_KEY;
        const AZURE_SPEECH_REGION = process.env.AZURE_SPEECH_REGION;
        
        console.log('[WebSocket] üîë Checking Azure credentials...');
        console.log('[WebSocket] Key present:', !!AZURE_SPEECH_KEY);
        console.log('[WebSocket] Region present:', !!AZURE_SPEECH_REGION);
        
        if (!AZURE_SPEECH_KEY || !AZURE_SPEECH_REGION) {
          console.log('[WebSocket] ‚ùå Azure credentials missing!');
          ws.send(JSON.stringify({ type: 'error', error: 'Azure Speech credentials missing!' }));
          ws.close();
          return;
        }
        
        console.log('[WebSocket] ‚úÖ Azure credentials OK, proceeding...');
`;

// Replace the connection handler
const connectionPattern = /wsServer\.on\('connection', \(ws\) => \{[\s\S]*?console\.log\('\[WebSocket\] üîó New client connected'\);[\s\S]*?try \{[\s\S]*?const AZURE_SPEECH_KEY = process\.env\.AZURE_SPEECH_KEY;[\s\S]*?console\.log\('\[WebSocket\] ‚úÖ Azure credentials OK, proceeding\.\.\.'\);/;
const connectionReplacement = connectionLogging;

if (connectionPattern.test(serverContent)) {
  serverContent = serverContent.replace(connectionPattern, connectionReplacement);
  console.log('‚úÖ Enhanced connection logging added to server');
} else {
  console.log('‚ùå Could not find connection handler pattern in server file');
}

// Write the improved server file
const improvedServerPath = path.join(__dirname, '..', 'azure-server-improved.js');
fs.writeFileSync(improvedServerPath, serverContent);

console.log('‚úÖ Improved server file created:', improvedServerPath);
console.log('üìã Changes made:');
console.log('   - Enhanced WebSocket message logging');
console.log('   - Added detailed audio data processing logs');
console.log('   - Added connection information logging');
console.log('   - Added error handling with detailed stack traces');
console.log('');
console.log('üöÄ To use the improved server:');
console.log('   1. Stop the current server');
console.log('   2. Replace azure-server.js with azure-server-improved.js');
console.log('   3. Restart the server');
console.log('   4. Test with the diagnostic tool'); 