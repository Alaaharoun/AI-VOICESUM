const express = require('express');
const cors = require('cors');
const multer = require('multer');
const fetch = require('node-fetch');
const fs = require('fs');
const { exec } = require('child_process');
const { promisify } = require('util');
const ffmpegPath = require('ffmpeg-static');
const WebSocket = require('ws');
const speechsdk = require('microsoft-cognitiveservices-speech-sdk');
const http = require('http');
require('dotenv').config();

const execAsync = promisify(exec);
const app = express();
app.use(cors());
app.use(express.json({ limit: '50mb' }));
const upload = multer();

// Environment variables
const AZURE_SPEECH_KEY = process.env.AZURE_SPEECH_KEY;
const AZURE_SPEECH_REGION = process.env.AZURE_SPEECH_REGION;

// Server startup logging
console.log('üöÄ AI Live Translate Server Starting...');
console.log('üìä Configuration:');
console.log('- Azure Speech Key:', AZURE_SPEECH_KEY ? '‚úÖ Present' : '‚ùå Missing');
console.log('- Azure Speech Region:', AZURE_SPEECH_REGION || '‚ùå Not set');
console.log('- Server Port:', process.env.PORT || 10000);

// ============================================================================
// LANGUAGE SUPPORT - All languages from the app
// ============================================================================

// Complete language mapping from the app
const AZURE_LANGUAGE_MAP = {
  'ar': 'ar-SA', 'en': 'en-US', 'es': 'es-ES', 'fr': 'fr-FR', 'de': 'de-DE',
  'it': 'it-IT', 'pt': 'pt-BR', 'ru': 'ru-RU', 'ja': 'ja-JP', 'ko': 'ko-KR',
  'zh': 'zh-CN', 'tr': 'tr-TR', 'nl': 'nl-NL', 'pl': 'pl-PL', 'sv': 'sv-SE',
  'da': 'da-DK', 'no': 'no-NO', 'fi': 'fi-FI', 'cs': 'cs-CZ', 'sk': 'sk-SK',
  'hu': 'hu-HU', 'ro': 'ro-RO', 'bg': 'bg-BG', 'hr': 'hr-HR', 'sl': 'sl-SI',
  'et': 'et-EE', 'lv': 'lv-LV', 'lt': 'lt-LT', 'el': 'el-GR', 'he': 'he-IL',
  'th': 'th-TH', 'vi': 'vi-VN', 'id': 'id-ID', 'ms': 'ms-MY', 'fil': 'fil-PH',
  'hi': 'hi-IN', 'bn': 'bn-IN', 'ur': 'ur-PK', 'fa': 'fa-IR', 'uk': 'uk-UA'
};

// Convert language code to Azure format
function convertToAzureLanguage(langCode) {
  const azureCode = AZURE_LANGUAGE_MAP[langCode];
  if (!azureCode) {
    console.warn(`‚ö†Ô∏è Unsupported language code: ${langCode}, defaulting to en-US`);
    return 'en-US';
  }
  console.log(`üåê Language conversion: ${langCode} ‚Üí ${azureCode}`);
  return azureCode;
}

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

// Convert MIME type to file extension
function mimeToExtension(mimeType) {
  if (mimeType.includes('webm')) return '.webm';
  if (mimeType.includes('ogg')) return '.ogg';
  if (mimeType.includes('mp3')) return '.mp3';
  if (mimeType.includes('wav')) return '.wav';
  if (mimeType.includes('m4a')) return '.m4a';
  if (mimeType.includes('pcm')) return '.raw';
  return '.bin';
}

// Convert audio format to WAV
async function convertAudioFormat(audioBuffer, inputFormat) {
  try {
    console.log(`üîÑ Converting ${inputFormat} to WAV...`);
    
    const inputExtension = mimeToExtension(inputFormat);
    const inputFile = `/tmp/input_${Date.now()}${inputExtension}`;
    const outputFile = `/tmp/output_${Date.now()}.wav`;
    
    fs.writeFileSync(inputFile, audioBuffer);
    
    const ffmpegCommand = `${ffmpegPath} -i "${inputFile}" -acodec pcm_s16le -ar 16000 -ac 1 "${outputFile}" -y`;
    await execAsync(ffmpegCommand);
    
    const convertedBuffer = fs.readFileSync(outputFile);
    
    // Cleanup
    try {
      fs.unlinkSync(inputFile);
      fs.unlinkSync(outputFile);
    } catch (cleanupError) {
      console.warn('‚ö†Ô∏è Could not clean up temp files:', cleanupError.message);
    }
    
    console.log(`‚úÖ Conversion successful: ${audioBuffer.length} ‚Üí ${convertedBuffer.length} bytes`);
    return convertedBuffer;
    
  } catch (error) {
    console.error('‚ùå Audio conversion failed:', error.message);
    return audioBuffer; // Return original if conversion fails
  }
}

// ============================================================================
// HTTP ENDPOINTS
// ============================================================================

// Health check
app.get('/health', (req, res) => {
  res.json({
    status: 'ok',
    timestamp: new Date().toISOString(),
    azureKey: AZURE_SPEECH_KEY ? 'Present' : 'Missing',
    supportedLanguages: Object.keys(AZURE_LANGUAGE_MAP).length
  });
});

// Audio transcription endpoint
const SUPPORTED_AUDIO_TYPES = [
  'audio/wav', 'audio/x-wav', 'audio/wave', 'audio/mpeg', 'audio/mp3', 
  'audio/m4a', 'audio/x-m4a', 'audio/ogg', 'audio/webm', 'audio/flac', 
  'audio/mp4', 'audio/pcm'
];

app.post('/live-translate', upload.single('audio'), async (req, res) => {
  try {
    let audioBuffer, audioType;
    
    if (req.file) {
      audioBuffer = req.file.buffer;
      audioType = req.file.mimetype;
    } else if (req.body && req.body.audio && req.body.audioType) {
      audioBuffer = Buffer.from(req.body.audio, 'base64');
      audioType = req.body.audioType;
    } else {
      return res.status(400).json({ error: 'Missing audio data' });
    }
    
    if (!AZURE_SPEECH_KEY || !AZURE_SPEECH_REGION) {
      return res.status(500).json({ error: 'Azure Speech API not configured' });
    }
    
    if (!SUPPORTED_AUDIO_TYPES.includes(audioType)) {
      return res.status(400).json({ error: 'Unsupported audio format' });
    }
    
    console.log(`üéµ Processing audio: ${audioType}, ${audioBuffer.length} bytes`);
    
    const wavBuffer = await convertAudioFormat(audioBuffer, audioType);
    const language = req.body.language || req.query.language || 'en-US';
    
    const azureEndpoint = `https://${AZURE_SPEECH_REGION}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=${language}`;
    
    const azureRes = await fetch(azureEndpoint, {
      method: 'POST',
      headers: {
        'Ocp-Apim-Subscription-Key': AZURE_SPEECH_KEY,
        'Content-Type': 'audio/wav',
        'Accept': 'application/json'
      },
      body: wavBuffer
    });
    
    const azureData = await azureRes.json();
    const transcriptText = azureData.DisplayText || '';
    
    console.log(`‚úÖ Transcription: "${transcriptText}"`);
    res.json({ transcription: transcriptText });
    
  } catch (error) {
    console.error('‚ùå Live-translate error:', error);
    res.status(500).json({ error: 'Transcription failed' });
  }
});

// ============================================================================
// WEBSOCKET REAL-TIME STREAMING
// ============================================================================

let wsServer;

function startWebSocketServer(server) {
  wsServer = new WebSocket.Server({ server, path: '/ws' });
  console.log('üîå WebSocket server started on /ws');

  wsServer.on('connection', (ws) => {
    console.log('üîó New WebSocket client connected');
    
    if (!AZURE_SPEECH_KEY || !AZURE_SPEECH_REGION) {
      console.error('‚ùå Azure Speech credentials missing');
      ws.send(JSON.stringify({ type: 'error', error: 'Azure Speech credentials missing' }));
      ws.close();
      return;
    }

    let recognizer, pushStream, speechConfig, audioConfig;
    let language = 'en-US';
    let initialized = false;
    let autoDetection = false;
    let detectedLanguage = null;

    ws.on('message', (data) => {
      try {
        // Handle JSON messages (ping, init, etc.)
        try {
          const msg = JSON.parse(data.toString());
          
          if (msg.type === 'ping') {
            ws.send(JSON.stringify({ type: 'pong' }));
            return;
          }
          
          if (!initialized && msg.type === 'init') {
            // Support both 'language' and 'sourceLanguage' fields for compatibility
            const sourceLanguage = msg.language || msg.sourceLanguage || 'auto';
            // Only enable auto-detection if explicitly requested or if language is 'auto'
            autoDetection = (sourceLanguage === 'auto') || (msg.autoDetection === true);
            
            console.log(`üåê Initializing with language: ${sourceLanguage}, auto-detection: ${autoDetection}`);
            
            // Create Azure Speech configuration
            const audioFormat = speechsdk.AudioStreamFormat.getWaveFormatPCM(16000, 16, 1);
            pushStream = speechsdk.AudioInputStream.createPushStream(audioFormat);
            audioConfig = speechsdk.AudioConfig.fromStreamInput(pushStream);
            
            speechConfig = speechsdk.SpeechConfig.fromSubscription(AZURE_SPEECH_KEY, AZURE_SPEECH_REGION);
            speechConfig.enableDictation();
            
            // Improved auto-detection setup with better error handling
            if (autoDetection) {
              console.log('üß† Auto Language Detection Enabled');
              
              try {
                // First try with a minimal set of languages for better compatibility
                const autoDetectLanguages = ["en-US", "ar-SA", "fr-FR", "es-ES", "de-DE"];
                
                // Create auto-detect config with proper error handling
                const autoDetectConfig = speechsdk.AutoDetectSourceLanguageConfig.fromLanguages(autoDetectLanguages);
                
                // Create recognizer with proper configuration order
                recognizer = new speechsdk.SpeechRecognizer(speechConfig, autoDetectConfig, audioConfig);
                console.log('‚úÖ AutoDetect recognizer created successfully');
                
              } catch (error) {
                console.error('‚ùå Failed to create AutoDetect recognizer:', error);
                
                // Try alternative approach with different language set
                try {
                  console.log('üîÑ Trying alternative auto-detection setup...');
                  const alternativeLanguages = ["en-US", "ar-SA"];
                  const altAutoDetectConfig = speechsdk.AutoDetectSourceLanguageConfig.fromLanguages(alternativeLanguages);
                  recognizer = new speechsdk.SpeechRecognizer(speechConfig, altAutoDetectConfig, audioConfig);
                  console.log('‚úÖ Alternative AutoDetect recognizer created successfully');
                  
                } catch (altError) {
                  console.error('‚ùå Alternative auto-detection also failed:', altError);
                  
                  // Final fallback to specific language
                  console.log('üîÑ Final fallback to en-US recognizer');
                  speechConfig.speechRecognitionLanguage = 'en-US';
                  recognizer = new speechsdk.SpeechRecognizer(speechConfig, audioConfig);
                  autoDetection = false;
                }
              }
            } else {
              // Specific language mode
              const azureLanguage = convertToAzureLanguage(sourceLanguage);
              speechConfig.speechRecognitionLanguage = azureLanguage;
              recognizer = new speechsdk.SpeechRecognizer(speechConfig, audioConfig);
              console.log(`‚úÖ Specific language recognizer created: ${azureLanguage}`);
              console.log(`üéØ Using specific language: ${sourceLanguage} ‚Üí ${azureLanguage}`);
              console.log(`üéØ Using specific language: ${sourceLanguage} ‚Üí ${azureLanguage}`);
            }
            
            // Event handlers
            recognizer.recognizing = (s, e) => {
              if (e.result.text && e.result.text.trim()) {
                // Extract detected language for auto-detection mode
                if (autoDetection && e.result.properties) {
                  detectedLanguage = e.result.properties.getProperty(speechsdk.PropertyId.SpeechServiceConnection_AutoDetectSourceLanguageResult);
                  console.log(`üé§ [AUTO‚Üí${detectedLanguage || 'detecting...'}] Recognizing: "${e.result.text}"`);
                } else {
                  console.log(`üé§ [${sourceLanguage}] Recognizing: "${e.result.text}"`);
                }
                
                ws.send(JSON.stringify({ 
                  type: 'transcription', 
                  text: e.result.text,
                  isPartial: true,
                  detectedLanguage: detectedLanguage
                }));
              }
            };
            
            recognizer.recognized = (s, e) => {
              if (e.result.reason === speechsdk.ResultReason.RecognizedSpeech && e.result.text) {
                // Extract detected language for final result
                if (autoDetection && e.result.properties) {
                  detectedLanguage = e.result.properties.getProperty(speechsdk.PropertyId.SpeechServiceConnection_AutoDetectSourceLanguageResult);
                  console.log(`‚úÖ [AUTO‚Üí${detectedLanguage}] Final: "${e.result.text}"`);
                } else {
                  console.log(`‚úÖ [${sourceLanguage}] Final: "${e.result.text}"`);
                }
                
                ws.send(JSON.stringify({ 
                  type: 'final', 
                  text: e.result.text,
                  isPartial: false,
                  detectedLanguage: detectedLanguage
                }));
              } else if (e.result.reason === speechsdk.ResultReason.NoMatch) {
                console.log('‚ö™ No speech could be recognized');
                ws.send(JSON.stringify({ 
                  type: 'final', 
                  text: '',
                  reason: 'NoMatch',
                  detectedLanguage: detectedLanguage
                }));
              }
            };
            
            recognizer.canceled = (s, e) => {
              console.error('‚ùå Recognition canceled:', e.errorDetails);
              
              // Check if it's a network-related error
              const isNetworkError = e.errorDetails && (
                e.errorDetails.includes('network') || 
                e.errorDetails.includes('Unable to contact server') ||
                e.errorDetails.includes('StatusCode: 1002') ||
                e.errorDetails.includes('StatusCode: 0')
              );
              
              if (isNetworkError) {
                console.log('üåê Network error detected, attempting to reconnect...');
                // Try to restart recognition after a short delay
                setTimeout(() => {
                  if (recognizer && ws.readyState === ws.OPEN) {
                    try {
                      recognizer.startContinuousRecognitionAsync(
                        () => console.log('‚úÖ Recognition restarted after network error'),
                        (err) => console.error('‚ùå Failed to restart recognition:', err)
                      );
                    } catch (restartError) {
                      console.error('‚ùå Error restarting recognition:', restartError);
                    }
                  }
                }, 2000);
              }
              
              ws.send(JSON.stringify({ 
                type: 'error', 
                error: `Recognition canceled: ${e.errorDetails}`,
                reason: e.reason,
                errorCode: e.errorCode,
                isNetworkError: isNetworkError
              }));
            };
            
            // Start recognition
            recognizer.startContinuousRecognitionAsync(
              () => {
                console.log('‚úÖ Continuous recognition started');
                initialized = true;
                ws.send(JSON.stringify({ 
                  type: 'ready', 
                  message: 'Ready for audio',
                  autoDetection: autoDetection
                }));
              },
              (err) => {
                console.error('‚ùå Failed to start recognition:', err);
                ws.send(JSON.stringify({ type: 'error', error: `Failed to start: ${err}` }));
              }
            );
            return;
          }
          
          if (msg.type === 'audio' && initialized) {
            // Handle audio data
            const audioBuffer = Buffer.from(msg.data, 'base64');
            const audioFormat = msg.format || 'audio/webm';
            
            console.log(`üéµ Received audio: ${audioBuffer.length} bytes, ${audioFormat}`);
            
            try {
              if (audioFormat === 'audio/pcm') {
                // Direct PCM data - ensure it's in the correct format
                if (audioBuffer.length > 0) {
                  pushStream.write(audioBuffer);
                }
              } else {
                // Convert other formats to WAV
                convertAudioFormat(audioBuffer, audioFormat)
                  .then(wavBuffer => {
                    if (wavBuffer && wavBuffer.length > 0) {
                      pushStream.write(wavBuffer);
                    }
                  })
                  .catch(error => {
                    console.error('‚ùå Audio processing error:', error);
                    ws.send(JSON.stringify({ 
                      type: 'error', 
                      error: 'Audio processing failed' 
                    }));
                  });
              }
            } catch (audioError) {
              console.error('‚ùå Audio write error:', audioError);
              ws.send(JSON.stringify({ 
                type: 'error', 
                error: 'Audio write failed' 
              }));
            }
          }
          
        } catch (jsonError) {
          // Handle raw audio data
          if (initialized && pushStream) {
            console.log(`üéµ Received raw audio: ${data.length} bytes`);
            pushStream.write(data);
          }
        }
        
      } catch (error) {
        console.error('‚ùå WebSocket message error:', error);
        ws.send(JSON.stringify({ type: 'error', error: 'Message handling failed' }));
      }
    });

    ws.on('close', () => {
      console.log('üîå WebSocket connection closed');
      if (recognizer) {
        recognizer.stopContinuousRecognitionAsync(() => {
          recognizer.close();
        });
      }
      if (pushStream) pushStream.close();
      if (speechConfig) speechConfig.close();
    });

    ws.on('error', (err) => {
      console.error('‚ùå WebSocket error:', err.message);
      if (recognizer) recognizer.close();
      if (pushStream) pushStream.close();
      if (speechConfig) speechConfig.close();
      ws.close();
    });

    // Add connection timeout
    const connectionTimeout = setTimeout(() => {
      if (!initialized) {
        console.log('‚è∞ Connection timeout - closing inactive connection');
        ws.close();
      }
    }, 30000); // 30 seconds timeout

    ws.on('close', () => {
      clearTimeout(connectionTimeout);
      console.log('üîå WebSocket connection closed');
      if (recognizer) {
        recognizer.stopContinuousRecognitionAsync(() => {
          recognizer.close();
        });
      }
      if (pushStream) pushStream.close();
      if (speechConfig) speechConfig.close();
    });
  });
}

// ============================================================================
// SERVER STARTUP
// ============================================================================

const server = http.createServer(app);
startWebSocketServer(server);

const PORT = process.env.PORT || 10000;
server.listen(PORT, () => {
  console.log(`üöÄ AI Live Translate Server running on port ${PORT}`);
  console.log(`üì° HTTP endpoints: http://localhost:${PORT}`);
  console.log(`üîå WebSocket: ws://localhost:${PORT}/ws`);
  console.log(`üíö Health check: http://localhost:${PORT}/health`);
  console.log(`üåç Supported languages: ${Object.keys(AZURE_LANGUAGE_MAP).length}`);
  console.log('‚ú® Server ready for connections!');
});