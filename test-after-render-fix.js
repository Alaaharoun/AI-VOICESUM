const WebSocket = require('ws');

console.log('ğŸ”§ Test After Render Fix - Verify Azure Speech Integration');

// Create realistic audio data for Azure Speech SDK (16kHz, 16-bit, mono PCM)
function createRealisticAudio(durationMs = 1000) {
  const sampleRate = 16000;
  const bitsPerSample = 16;
  const channels = 1;
  const bytesPerSample = bitsPerSample / 8;
  const totalSamples = Math.floor(sampleRate * (durationMs / 1000));
  const buffer = Buffer.alloc(totalSamples * bytesPerSample * channels);
  
  // Generate a sine wave at 440Hz (A note) - more realistic than silence
  for (let i = 0; i < totalSamples; i++) {
    const time = i / sampleRate;
    const frequency = 440; // A note
    const amplitude = 0.3; // Moderate volume
    const sample = Math.sin(2 * Math.PI * frequency * time) * amplitude * 32767;
    buffer.writeInt16LE(Math.round(sample), i * 2);
  }
  
  console.log(`ğŸµ Generated ${durationMs}ms audio: ${buffer.length} bytes, ${totalSamples} samples @ ${sampleRate}Hz`);
  return buffer;
}

function testRenderAfterFix() {
  console.log('\nğŸ“‹ Testing Render Server After Azure Credentials Fix');
  
  const ws = new WebSocket('wss://ai-voicesum.onrender.com/ws');
  let receivedMessages = [];
  let hasCredentials = false;
  let hasTranscription = false;
  
  ws.on('open', () => {
    console.log('âœ… WebSocket connected to Render server');
    
    // Real-time mode init message (same as live-translation.tsx)
    const initMessage = {
      type: 'init',
      language: 'en-US',
      targetLanguage: 'ar-SA',
      clientSideTranslation: true,
      realTimeMode: true,
      autoDetection: true,
      audioConfig: {
        sampleRate: 16000,
        channels: 1,
        bitsPerSample: 16,
        encoding: 'pcm_s16le'
      }
    };
    
    console.log('ğŸ“¤ Sending init message (same as live-translation.tsx):', JSON.stringify(initMessage, null, 2));
    ws.send(JSON.stringify(initMessage));
    
    // Wait for initialization, then send audio
    setTimeout(() => {
      if (hasCredentials) {
        console.log('ğŸ“¤ Sending first audio chunk...');
        const audio1 = createRealisticAudio(1000); // 1 second
        ws.send(audio1);
      }
    }, 3000);
    
    setTimeout(() => {
      if (hasCredentials) {
        console.log('ğŸ“¤ Sending second audio chunk...');
        const audio2 = createRealisticAudio(1500); // 1.5 seconds
        ws.send(audio2);
      }
    }, 5000);
    
    setTimeout(() => {
      console.log('â° Test completed - analyzing Render server results');
      console.log(`ğŸ“Š Total messages received: ${receivedMessages.length}`);
      
      if (receivedMessages.length === 0) {
        console.log('âŒ STILL NO MESSAGES - Azure credentials still missing on Render');
        console.log('   ğŸ“‹ Action needed: Add AZURE_SPEECH_KEY and AZURE_SPEECH_REGION to Render');
      } else {
        console.log('âœ… Messages received from Render server:');
        receivedMessages.forEach((msg, i) => {
          console.log(`   ${i + 1}. ${msg.type}: "${msg.text || msg.message || msg.error}"`);
        });
        
        if (hasCredentials && hasTranscription) {
          console.log('ğŸ‰ SUCCESS! Azure Speech is working on Render!');
          console.log('   ğŸ”„ Real-time transcription will now work in the app');
          console.log('   ğŸ“± Live translation feature is fully operational');
        } else if (hasCredentials && !hasTranscription) {
          console.log('âš ï¸ Credentials fixed but no transcription yet');
          console.log('   ğŸ’¡ This is normal with generated audio - try with real voice');
        } else {
          console.log('âŒ Credentials still missing - check Render environment variables');
        }
      }
      
      ws.close();
    }, 10000);
  });
  
  ws.on('message', (data) => {
    try {
      const message = JSON.parse(data.toString());
      receivedMessages.push(message);
      console.log('ğŸ“¥ Render Server Response:', message.type, '-', message.text || message.message || message.error);
      
      if (message.type === 'error') {
        if (message.error.includes('credentials')) {
          console.error('âŒ Credentials Error - still not fixed on Render:', message.error);
        } else {
          console.error('âŒ Other Error:', message.error);
        }
      } else if (message.type === 'status') {
        if (message.message.includes('Ready for audio') || message.message.includes('session started')) {
          hasCredentials = true;
          console.log('âœ… Azure credentials working - server is ready!');
        }
        console.log('â„¹ï¸ Status update:', message.message);
      } else if (message.type === 'final' || message.type === 'transcription') {
        hasTranscription = true;
        console.log('ğŸ¤ Speech recognition result:', message);
        console.log('   ğŸ”„ This would appear immediately in live-translation.tsx');
      }
    } catch (e) {
      console.log('ğŸ“¥ Render Server Raw Response:', data.toString());
    }
  });
  
  ws.on('close', (code, reason) => {
    console.log(`ğŸ”’ Render server connection closed: ${code} - ${reason}`);
    
    console.log('\nğŸ“‹ Summary:');
    if (hasCredentials) {
      console.log('âœ… Azure credentials are working on Render');
      if (hasTranscription) {
        console.log('âœ… Speech recognition is working');
        console.log('ğŸ‰ Live translation feature is ready!');
      } else {
        console.log('âš ï¸ No transcription (normal with generated audio)');
        console.log('ğŸ’¡ Test with real voice recording for best results');
      }
    } else {
      console.log('âŒ Azure credentials still missing on Render');
      console.log('ğŸ“‹ Next steps:');
      console.log('   1. Go to Render Dashboard');
      console.log('   2. Navigate to Environment Variables');
      console.log('   3. Add AZURE_SPEECH_KEY and AZURE_SPEECH_REGION');
      console.log('   4. Redeploy the service');
    }
    
    console.log('\nâœ¨ Test completed!');
  });
  
  ws.on('error', (err) => {
    console.log('âŒ Render Server Error:', err.message);
  });
}

// Instructions
console.log('ğŸš€ This test verifies if Azure Speech credentials have been added to Render\n');
console.log('ğŸ“‹ Expected flow after credentials are fixed:');
console.log('   1. WebSocket connects successfully');
console.log('   2. Init message is sent');
console.log('   3. Server responds with "Recognition session started"');
console.log('   4. Server responds with "Ready for audio input"');
console.log('   5. Audio chunks are sent');
console.log('   6. Server may respond with transcription results\n');

// Start test
testRenderAfterFix(); 